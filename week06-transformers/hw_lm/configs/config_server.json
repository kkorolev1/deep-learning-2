{
  "name": "default_config",
  "n_gpu": 0,
  "arch": {
    "type": "HuYaLM",
    "args": {
      "vocab_size": 5000,
      "embed_dim": 512,
      "feedforward_dim": 2048,
      "num_heads": 8,
      "num_layers": 8,
      "max_len": 320
    }
  },
  "dataset": {
    "padding_mode": "truncate",
    "seq_len": 256,
    "data_dir": "TinyStoriesTokenized",
    "tokenizer_model_path": "lm.model",
    "val_size": 0.01
  },
  "data": {
    "train": {
      "batch_size": 256,
      "num_workers": 5,
      "datasets": [
        {
          "type": "TinyStoriesDataset",
          "args": {
            "is_test": false
          }
        }
      ]
    },
    "val": {
      "batch_size": 256,
      "num_workers": 5,
      "datasets": [
        {
          "type": "TinyStoriesDataset",
          "args": {
            "is_test": true
          }
        }
      ]
    }
  },
  "optimizer": {
    "type": "AdamW",
    "args": {
      "lr": 1e-3,
      "weight_decay": 1e-5
    }
  },
  "lr_scheduler": {
    "type": "OneCycleLR",
    "args": {
      "steps_per_epoch": 2500,
      "epochs": 50,
      "anneal_strategy": "cos",
      "max_lr": 1e-3,
      "pct_start": 0.2
    }
  },
  "loss": {
    "type": "CrossEntropyLoss",
    "args": {
      "pad_id": 0
    }
  },
  "metrics": [],
  "trainer": {
    "epochs": 50,
    "save_dir": "saved/",
    "save_period": 5,
    "verbosity": 2,
    "monitor": "min val_loss",
    "early_stop": 100,
    "visualize": "wandb",
    "wandb_project": "lm_project",
    "wandb_run_name": "one_batch_test",
    "len_epoch": 5000,
    "grad_norm_clip": 100,
    "grad_accum_iters": 2
  }
}