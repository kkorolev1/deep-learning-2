{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korolevki/.conda/envs/lm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw_lm.tokenizer import Tokenizer\n",
    "from hw_lm.model import HuYaLM\n",
    "\n",
    "mytokenizer = Tokenizer(\"lm.model\")\n",
    "mymodel = HuYaLM(\n",
    "    vocab_size=5000,\n",
    "    embed_dim=512,\n",
    "    feedforward_dim=2048,\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    max_len=320\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(\"lm_epochs_21.pth\")\n",
    "mymodel.load_state_dict(checkpoint[\"state_dict\"])\n",
    "mymodel.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time Aiza killed all her friends\n",
      " One day, they decided to play a game\n",
      " They all went to the park, and when they arrived, the kit was empty\n",
      " There was nothing to do! Aiza started to cry\n",
      " \"Why are you crying?\" asked a voice\n",
      " Aiza looked up and saw a man smiling at her\n",
      " He said, \"I'm so sorry\n",
      " I didn't know you were going to do something to make you happy\"\n",
      " Aiza's friends were very compassionate, and they said, \"It's ok! We were just playing a game\n",
      "\" The man smiled and said, \"Well, I'm sorry I was being so mean\n",
      " I have a special surprise for you\n",
      "\" He then reached into his pocket and pulled out a shiny object\n",
      " Aiza looked at him and asked, \"What is it?\" The man said, \"It's a toy car\n",
      " I put it in my pockets\n",
      " You can play with it if you want\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "text = mymodel.inference(\n",
    "    \"Once upon a time\",\n",
    "    mytokenizer,\n",
    "    device,\n",
    "    max_length=200,\n",
    "    temperature=1.3,\n",
    "    mode=\"nucleus\",\n",
    "    p=0.9\n",
    ")\n",
    "\n",
    "for s in text[0].split(\".\"):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Once upon a time there was a girl that was like me, and my father wanted me to marry a rich and noble family because there was a chance I could become a powerful sorcerer and save the kingdom, but because he wanted to get me married to'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import perplexity\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bos_eos(input_ids):\n",
    "    return torch.cat((\n",
    "        torch.tensor([mytokenizer.processor.bos_id()]),\n",
    "        input_ids,\n",
    "        torch.tensor([mytokenizer.processor.eos_id()])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korolevki/.conda/envs/lm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from tqdm.auto import tqdm\n",
    "with open(\"json_data/data00.json\") as f:\n",
    "    data = json.load(f)\n",
    "    data = [x[\"story\"].strip() for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycollate(batch):\n",
    "    input_ids = pad_sequence([\n",
    "        add_bos_eos(torch.tensor(item[:256], dtype=torch.int64)) for item in batch], \n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    ).to(device)\n",
    "    padding_mask = input_ids == mytokenizer.processor.pad_id()\n",
    "    return input_ids, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 169.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINE tensor(2.9433, device='cuda:3', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def mycalculate(data):\n",
    "    bs = 1\n",
    "    mymetric = 0.0\n",
    "\n",
    "    for i in tqdm(range(len(data) // bs)):\n",
    "        encoded_data = mytokenizer.encode(data[i*bs:(i+1)*bs])\n",
    "        batch = mycollate(encoded_data)\n",
    "        mylogits = mymodel(*batch)[\"logits\"]\n",
    "        metric = perplexity(mylogits[:, :-1, :], batch[0][:, 1:], ignore_index=0)\n",
    "        mymetric += metric * bs\n",
    "\n",
    "    mymetric /= len(data)\n",
    "\n",
    "    print(\"MINE\", mymetric)\n",
    "    \n",
    "mycalculate(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-xl\")\n",
    "model.eval();\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [12:50<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THEIR tensor(6.7053, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def calculate(data):\n",
    "    bs = 1\n",
    "    mymetric = 0.0\n",
    "\n",
    "    for i in tqdm(range(len(data) // bs)):\n",
    "        batch = tokenizer(data[i*bs:(i+1)*bs], truncation=True, padding=\"max_length\", max_length=256)\n",
    "        batch = {k: torch.tensor(v) for k,v in batch.items()}\n",
    "        logits = model(**batch).logits\n",
    "        metric = perplexity(logits[:, :-1, :], batch[\"input_ids\"][:, 1:], ignore_index=tokenizer.eos_token_id)\n",
    "        mymetric += metric * bs\n",
    "\n",
    "    mymetric /= len(data)\n",
    "\n",
    "    print(\"THEIR\", mymetric)\n",
    "    \n",
    "calculate(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
